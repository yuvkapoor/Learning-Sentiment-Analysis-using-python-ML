{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1X4qnEDeYZcI7cEhdrEx-Rpd8XI43_qkF","authorship_tag":"ABX9TyM22H+AKKbv2CviAwWV8IC7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import string\n","from collections import Counter\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('vader_lexicon')\n","\n","text = open('/content/drive/MyDrive/Colab Notebooks/test.txt', encoding='utf-8').read()\n","lower_case = text.lower()\n","cleaned_text = lower_case.translate(str.maketrans('', '', string.punctuation))\n","\n","tokenized_words = word_tokenize(cleaned_text, \"english\")\n","\n","final_words = []\n","for word in tokenized_words:\n","    if word not in stopwords.words('english'):\n","        final_words.append(word)\n","\n","lemma_words = []\n","for word in final_words:\n","    word = WordNetLemmatizer().lemmatize(word)\n","    lemma_words.append(word)\n","\n","emotion_list = []\n","with open('/content/drive/MyDrive/Colab Notebooks/emotions.txt', 'r') as file:\n","    for line in file:\n","        clear_line = line.replace(\"\\n\", '').replace(\",\", '').replace(\"'\", '').strip()\n","        word, emotion = clear_line.split(':')\n","\n","        if word in lemma_words:\n","            emotion_list.append(emotion)\n","\n","print(emotion_list,'\\n')\n","w = Counter(emotion_list,),'\\n'\n","print(w,'\\n')\n","\n","\n","def sentiment_analyse(sentiment_text):\n","    score = SentimentIntensityAnalyzer().polarity_scores(sentiment_text)\n","    if score['neg'] > score['pos']:\n","        print(\"Negative Sentiment\",'\\n')\n","    elif score['neg'] < score['pos']:\n","        print(\"Positive Sentiment\",'\\n')\n","    else:\n","        print(\"Neutral Sentiment\",'\\n')\n","\n","\n","sentiment_analyse(cleaned_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EAbeIgS6rUG6","executionInfo":{"status":"ok","timestamp":1686727242774,"user_tz":-330,"elapsed":392,"user":{"displayName":"Yuv Kapoor","userId":"04651123546459955797"}},"outputId":"d3e54260-bf5a-45eb-b340-12922867a9fc"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["[' happy', ' happy', ' attached', ' happy', ' attracted', ' alone', ' free', ' hated', ' happy', ' entitled', ' happy', ' loved', ' hated', ' entitled'] \n","\n","(Counter({' happy': 5, ' hated': 2, ' entitled': 2, ' attached': 1, ' attracted': 1, ' alone': 1, ' free': 1, ' loved': 1}), '\\n') \n","\n","Positive Sentiment \n","\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n"]}]}]}